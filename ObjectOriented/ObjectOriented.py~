import numpy as np
import os
import sys
from exactlearning import wrap, plots
from scipy.optimize import minimize
from scipy.spatial import KDTree

pwd = os.getcwd()
string_list = "a b c d e f g h i j k l m n o p q r s t u v w x y z".split(" ")

###########################
## Predefined complexity ##
###########################
## In order to best examine the spaces etc.
## We define sets of constants


### Notes for extensions... 

## Where we have phi(s) = C K^s P_k(s)/Q_l(s) * Gamma()/Gamma() product
## With P and Q as polynomials (i.e. Quantum Mechanics Examples)
## We can isolate a quantity

## phi'(s)/phi(s) + Q'(s)/Q(s) - P'(s)/P(s) = log(K) + Sum( digamma(...) )
## I.e. we can insert the polynomial terms to the other side as well

## This allows the logd estimator to be used for polynomial terms

## In general... any new term added by product to the fingerprint i.e. zeta(s)Gamma(s)
## When the logarithmic derivative is taken, we get the extra term, so this can be very general
## phi'(s)/phi(s) + zeta'(s)/zeta(s)





## A dict of integers and rationals
rationals_dict = {}

keys = []
values = []
## Generate a dictionary of rationals
for i in range(20):
  for j in range(1,20):
    q = i/j
    st = "{}/{}".format(i,j)
    stlen = len(st)
    if(q not in values):
      values.append(q)
      keys.append(st)
      continue
    if(q in values):
      arg = np.argwhere(np.array(values)==q)[0][0]
      if(stlen<len(keys[arg])):
        values.pop(arg)
        keys.pop(arg)
        values.append(q)
        keys.append(st)

rationals_dict = {i:j for i,j in zip(keys,values)}

## A dict of only integers (useful for powers of gamma functions)
integers_dict = { "{}".format(i) : i for i in range(100) }

#non_zero_flexible_constants = [1,2,3]
#gamma_rational_constants = [-1,-2,-3,0,1,2,3,1/2,1/3,2/3] 
#hypergeometric_arguments = [-1,1,1/2,1/(2**2),(2**2)/(3**3),(3**3)/(4**4)] 

## A flexible list of possible prefactor type constants including gamma functions, roots and Pi
not_zero_dict = rationals_dict.copy()

## Sqrt of these constants for positive only BFGS varaibles
sqrt_not_zero_dict = { "sqrt({})".format(i) : np.sqrt(not_zero_dict[i]) for i in not_zero_dict.keys()}

## Hypergeometric arguments, should include 2**2/3**3 type numbers as well
hyp_arg_dict = rationals_dict.copy()

## For polynomial coeffieicnts (usually rationals)
poly_coeff_dict = rationals_dict.copy()


## A global list of constants, this is used for key matching
global_constants_dict = { "_notzero_" : not_zero_dict, "_sqrtnotzero_" : sqrt_not_zero_dict, "_gamma-rational_" : rationals_dict, "_hyp-arg_" : hyp_arg_dict, "poly-coeff" : poly_coeff_dict }

## This is a dict of trees for lookup of each type of constant
global_trees_dict = { i : KDTree([ [j] for j in global_constants_dict[i].values() ]) for i in global_constants_dict.keys() }


## Add a square to insist on positive log
constant_dict = {
"n" : 1, 
"req" : "", 
"logreq" : "from numpy import log", 
"moment" : "_notzero_", 
"logmoment" : "log(_sqrtnotzero_**2)",
"logderivative" : "0",
"logderivativereq" : "",
"logderivative2" : "0",
"logderivative2req" : ""
}

power_dict = {
"n" : 1, 
"req" : "", 
"logreq" : "from numpy import log",
"moment" : "_notzero_**_s_", 
"logmoment" : "_s_*log(_sqrtnotzero_**2)",
"logderivative" : "log(_sqrtnotzero_**2)",
"logderivativereq" : "from numpy import log",
"logderivative2" : "0",
"logderivative2req" : ""
}

linear_gamma_dict = {
"n" : 2, 
"req" : "from scipy.special import gamma", 
"logreq" : "from scipy.special import loggamma", 
"moment" : "gamma(_gamma-rational_ + _s_*_gamma-rational_)", 
"logmoment" : "loggamma(_gamma-rational_ + _s_*_gamma-rational_)",
"logderivative" : "_VAR2_*digamma(_gamma-rational_ + _s_*_gamma-rational_)",
"logderivativereq" : "from scipy.special import digamma",
"logderivative2" : "_VAR2_**2*np.trigamma(_gamma-rational_ + _s_*_gamma-rational_)",
#"logderivative2req" : "from mpmath import psi"
"logderivative2req" : "from AdvancedFunctions import *"
}

scale_gamma_dict = {
        "n" : 1, 
        "req" : "from scipy.special import gamma", 
        "logreq" : "from scipy.special import loggamma", 
        "moment" : "gamma(_s_*_gamma-rational_)", 
        "logmoment" : "loggamma(_s_*_gamma-rational_)"}

shift_gamma_dict = {
        "n" : 1, 
        "req" : "from scipy.special import gamma", 
        "logreq" : "from scipy.special import loggamma", 
        "moment" : "gamma(_gamma-rational_ + _s_)", 
        "logmoment" : "loggamma(_gamma-rational_ + _s_)"}

neg_linear_gamma_dict = {
"n" : 2, 
"req" : "from scipy.special import rgamma", 
"logreq" : "from scipy.special import loggamma", 
"moment" : "rgamma(_gamma-rational_ + _s_*_gamma-rational_)", 
"logmoment" : "-loggamma(_gamma-rational_ + _s_*_gamma-rational_)",
"logderivative" : "-_VAR2_*digamma(_gamma-rational_ + _s_*_gamma-rational_)",
"logderivativereq" : "from scipy.special import digamma",
#"logderivative2" : "-_VAR2_**2*array([complex(psi(1,_gamma-rational_ + ss*_gamma-rational_)) for ss in _s_])",
"logderivative2" : "-_VAR2_**2*np.trigamma(_gamma-rational_ + _s_*_gamma-rational_)",
#"logderivative2req" : "from mpmath import psi"
"logderivative2req" : "from AdvancedFunctions import *"
}

neg_scale_gamma_dict = {
        "n" : 1, 
        "req" : "from scipy.special import rgamma", 
        "logreq" : "from scipy.special import loggamma", 
        "moment" : "rgamma(_s_*_gamma-rational_)", 
        "logmoment" : "-loggamma(_s_*_gamma-rational_)"}

neg_shift_gamma_dict = {
        "n" : 1, 
        "req" : "from scipy.special import rgamma", 
        "logreq" : "from scipy.special import loggamma", 
        "moment" : "rgamma(_gamma-rational_ + _s_)", 
        "logmoment" : "-loggamma(_gamma-rational_ + _s_)"}

## We only have one constant and let the prefactor absorb the scaling
P1_dict = {
        "n" : 1, 
        "req" : "", 
        "logreq" : "from numpy import log", 
        "moment" : "(1 + _s_*_poly-coeff_)", 
        "logmoment" : "log(1 + _s_*_poly-coeff_)"}

P2_dict = {
        "n" : 2, 
        "req" : "", 
        "logreq" : "from numpy import log", 
        "moment" : "(1 + _s_*_poly-coeff_ + _s_**2*_poly-coeff_)", 
        "logmoment" : "log(1 + _s_*_poly-coeff_ + _s_**2*_poly-coeff_)"}

twoFone_dict = {
        "n" : 7, 
        "req" : "from mpmath import hyp2f1", 
        "logreq" : "from mpmath import hyp2f1\nfrom numpy import log", 
        "moment" : "hyp2f1(_gamma-rational_ + _s_*_gamma-rational_,_gamma-rational_ + _s_*_gamma-rational_,_gamma-rational_ + _s_*_gamma-rational_,_hyp-arg_)", 
        "logmoment" : "np.array([log(complex(hyp2f1(_gamma-rational_ + ss*_gamma-rational_,_gamma-rational_ + ss*_gamma-rational_,_gamma-rational_ + ss*_gamma-rational_,_hyp-arg_))) for ss in _s_])"}

oneFone_dict = {
        "n" : 5, 
        "req" : "from mpmath import hyp1f1", 
        "logreq" : "from mpmath import hyp1f1\nfrom numpy import log", 
        "moment" : "hyp1f1(_gamma-rational_ + _s_*_gamma-rational_,_gamma-rational_ + _s_*_gamma-rational_,_hyp-arg_)", 
        "logmoment" : "np.array([log(complex(hyp1f1(_gamma-rational_ + ss*_gamma-rational_,_gamma-rational_ + ss*_gamma-rational_,_hyp-arg_))) for ss in _s_])"}


## Somehow need to figure out the same parameters ?
pi_csc_pi_dict = {"n":2, "rules" : {"_A_" : "_rationals_", "_B_" : "_rationals_"}, "moment" : "(np.pi/_A_)*np.csc(np.pi*_s_/_A_ + np.pi/_B_)"}

## Consider a sum of the above units (this appears in elliptic K for example)

## possibly make this safe with mpmath gammaprod
norm_shift_gamma_dict = {"n" : 1,"rules" : {"_A_" : "_gamma-rational_"}, "req" : "from scipy.special import gamma, rgamma", "logreq" : "from scipy.special import loggamma","moment" : "gamma(_A_ + _s_)*rgamma(_A_)", "logmoment" : "loggamma(_A_ + _s_)-loggamma(_A_)" }  ## With rules

neg_norm_shift_gamma_dict = {"n" : 1,"rules" : {"_A_" : "_gamma-rational_"}, "req" : "from scipy.special import gamma, rgamma", "logreq" : "from scipy.special import loggamma","moment" : "rgamma(_A_ + _s_)*gamma(_A_)", "logmoment" : "-loggamma(_A_ + _s_)+loggamma(_A_)" }  ## With rules

norm_scale_gamma_dict = {"n" : 2,"rules" : {"_A_" : "_gamma-rational_", "_B_" : "_gamma-rational_"}, "req" : "from scipy.special import gamma, rgamma", "logreq" : "from scipy.special import loggamma","moment" : "_B_*gamma(_A_ + _B_*_s_)*rgamma(_A_)", "logmoment" : "log(_B_) + loggamma(_A_ + _B_*_s_)-loggamma(_A_)" }  ## With rules

neg_norm_scale_gamma_dict = {"n" : 2,"rules" : {"_A_" : "_gamma-rational_", "_B_" : "_gamma-rational_"}, "req" : "from scipy.special import gamma, rgamma", "logreq" : "from scipy.special import loggamma","moment" : "(1/_B_)*rgamma(_A_ + _B_*_s_)*gamma(_A_)", "logmoment" : "-log(_B_) - loggamma(_A_ + _B_*_s_)+loggamma(_A_)" }  ## With rules

## For two linear gammas?
linear_beta_dict = {"n": 2, "rules" : {"_A_"}, }

Erdelyi_G_function = {} ## With shift etc.
Higher_Polygamma_type_functions = {} ### 1,2,3,

## Window fitting ####################
## If we have an integral between [alpha,beta], this is the Mellin transform with Theta[x-alpha]*Theta[beta-x]
## as a window. For a hypergeometric result this gives
## (beta^s/s) (p+1)F(q+1)( ..., s; ..., 1+s; -beta) - (alpha^s/s) (p+1)F(q+1)( ..., s; ..., 1+s; -alpha)
## So by keeping the params the same and searching for a representation we can possibly identify the interior function
## with only a partial curve!
#######################################
## In addition to this, we can use differences in incomplete gamma functions?
################


### HyperU
### Generalised Hyper helper
### HyperComb()
### Special Polynomials? (i.e. Gegenbauer, Jacobi, order n+1/2 etc.)
### Zeta(s)


## FINISH
def generate_hyper_pFq(p,q):
  f_dict = {"0|1":"hyp0f1","1|1":"hyp1f1","1|2":"hyp1f2","2|0":"hyp2f0","2|1":"hyp2f1","2|2":"hyp2f2","2|3":"hyp2f3","3|2":"hyp3f2"}
  string_p = "["+",".join(["_gamma-rational_ + ss*_gamma-rational_" for i in range(p)])+"]"
  string_q = "["+",".join(["_gamma-rational_ + ss*_gamma-rational_" for i in range(q)])+"]"
  key = "{}|{}".format(p,q)
  if(key in f_dict.keys()):
    f_string = f_dict[key]
  ##  string = "{}({},{},_hyp-arg_)".format(f_string,str...)
  else: 
    f_string = "hyper"
  #return {"n" : 2*p+2*q+1, "req" : "from mpmath import {}".format(f_string), "logreq" : "from mpmath import {}\nfrom numpy import log".format(f_string), "moment" : "[complex({}) for ss in _s_]".format(string), "logmoment" : "np.array([log(complex({})) for ss in _s_])".format(string)}



## A helper function to enumerate many possible Meijer-G function combinations
def generate_meijerg_dict(a,b,c,d):
  string_a = "["+",".join(["_gamma-rational_ + ss*_gamma-rational_" for i in range(a)])+"]"
  string_b = "["+",".join(["_gamma-rational_ + ss*_gamma-rational_" for i in range(b)])+"]"
  string_c = "["+",".join(["_gamma-rational_ + ss*_gamma-rational_" for i in range(c)])+"]"
  string_d = "["+",".join(["_gamma-rational_ + ss*_gamma-rational_" for i in range(d)])+"]"
  string = "meijerg(({},{}),({},{}),_hyp-arg_)".format(string_a,string_b,string_c,string_d)
  return {"n" : 2*a+2*b+2*c+2*d+1, "req" : "from mpmath import meijerg", "logreq" : "from mpmath import meijerg\nfrom numpy import log", "moment" : "[complex({}) for ss in _s_]".format(string), "logmoment" : "np.array([log(complex({})) for ss in _s_])".format(string)}

## Conversion between shorthand 'elements' and content
## Each entry represents a term which can be added
term_dict = {
"c" : constant_dict,
"c^s" : power_dict,
"linear-gamma" : linear_gamma_dict,
"scale-gamma" : scale_gamma_dict,
"shift-gamma" : shift_gamma_dict,
"neg-linear-gamma" : neg_linear_gamma_dict,
"neg-scale-gamma" : neg_scale_gamma_dict,
"neg-shift-gamma" : neg_shift_gamma_dict,
"P1" : P1_dict,
"P2" : P2_dict,
"2F1" : twoFone_dict
}

## Generate the Meijer-G dicts
for a in range(0,4):
  for b in range(0,4):
    for c in range(0,4):
      for d in range(0,4):
        term_dict["G-{}-{}-{}-{}".format(a,b,c,d)]= generate_meijerg_dict(a,b,c,d)

### EOF Predefined Complexity ###

## A helper function to assign pk parameter labels to constant spaces and vice versa
def parse_terms(terms):
  terms = terms.replace("_s_","")
  term_array = np.array(list(terms))
  underscores = np.argwhere( term_array == "_")
  undercount = len(underscores)
  if(undercount%2 != 0):
    print("Error! : Bad underscore count in {}".format(terms))
    exit()
  underscores = np.reshape(underscores,(undercount//2,2))
  strings = [terms[a:b+1] for a,b in underscores]
  param_type_dict = { strings[k] : "p{}".format(k) for k in range(len(strings)) }
  return strings , ["p{}".format(k) for k in range(undercount//2)]

## The exact learning estimator class
## Not designed to take a lot of information
class ExactEstimator:
  def __init__(self,tag, folder = ""):
    self.tag = tag
    self.folder = folder
    self.sample_mode = "first"
    self.fit_mode = "log"
    self.N_terms = None

    self.BFGS_derivative_order = 0

    ## These are the object
    ## The fingerpint is the dict
    ## The function is a key that is a file (could eventually make a string) containing the function
    self.fingerprint = None
    self.function = None

    ## When functions are defined a record of their composition can be found here
    self.fingerprint_function_dict = {}
    self.function_terms_dict = {}

    ## Record the best results seen
    self.best_loss = np.inf
    self.best_params = None
    self.best_fingerprint = None
    self.best_function = None

    ## Record all results seen
    ## A dictionary of losses, hashed by
    self.results = {}

    ## Data
    self.s_values    = np.load("{}/s_values_{}.npy".format(folder,tag))
    self.sample_array = np.arange(self.s_values.shape[0])  ## This is for sampling from the samples
    self.moments     = np.load("{}/moments_{}.npy".format(folder,tag))
    self.logmoments  = np.log(self.moments)

    ## Load in the ratio dq/q which is expected to be a sum of digamma functions
    self.ratio = np.load("{}/logderivative_{}.npy".format(folder,tag))
    self.ratio2 = np.load("{}/logderivative2_{}.npy".format(folder,tag))

    ## If the errors are to be found
    if(os.path.exists("{}/real_error_{}.npy".format(folder,tag))):
      self.real_error = np.load("real_error_{}.npy".format(tag))
    else:
      self.real_error = np.zeros(self.moments.shape)
    
    if(os.path.exists("{}/imag_error_{}.npy".format(folder,tag))):
      self.imag_error = np.load("imag_error_{}.npy".format(tag))
    else:
      self.imag_error = np.zeros(self.moments.shape)
    

    self.real_s = np.real(self.s_values)
    self.imag_s = np.imag(self.s_values)
    self.real_m = np.real(self.moments)
    self.imag_m = np.imag(self.moments)
    self.real_logm = np.real(self.logmoments)
    self.imag_logm = np.imag(self.logmoments)
    
    ## Calculate the error margins 
    real_log_upper = np.real(np.log(self.moments + self.real_error))
    real_log_lower = np.real(np.log(self.moments - self.real_error))
    imag_log_upper = np.imag(np.log(self.moments + self.imag_error))
    imag_log_lower = np.imag(np.log(self.moments - self.imag_error))

    ## The bounds to work with when fitting
    self.real_log_diff = real_log_upper - real_log_lower
    self.imag_log_diff = imag_log_upper - imag_log_lower
 

  ## Write a vectorized function to evaluate arbitary fingerprint
  ## The method is passed a dictionary to do this
  def write_function(self,key):
    print("Writing function: ",self.fingerprint,key)
    params = self.fingerprint.split(":")[3:]

    if(self.fit_mode == "log"):
      requirements = set( term_dict[term]["logreq"] for term in params )
      terms = ":".join([ term_dict[term]["logmoment"] for term in params])
      logderivative_requirements = set( term_dict[term]["logderivativereq"] for term in params )
      logderivative_terms = ":".join([ term_dict[term]["logderivative"] for term in params])
      logderivative2_requirements = set( term_dict[term]["logderivative2req"] for term in params )
      logderivative2_terms = ":".join([ term_dict[term]["logderivative2"] for term in params])
      print(logderivative_terms)
      print(logderivative_requirements)
    if(self.fit_mode == "normal"): 
      requirements = set( term_dict[term]["req"] for term in params )
      terms = ":".join([ term_dict[term]["moment"] for term in params]) 
    keys, values = parse_terms(terms) 
    self.function_terms_dict[key] = {values[i]:keys[i] for i in range(len(keys))}

    ## Writing a file that will rapidly fit the fingerprint 
    #with open(".\\Functions\\"+key+".py","w") as f:
    with open("./Functions/"+key+".py","w") as f:
      for req in requirements: f.write(req+"\n")
      f.write("from numpy import array, frompyfunc\n")
      f.write("def fp({}):\n".format(",".join(["p{}".format(i) for i in range(self.N_terms)])))
      iterate = 0
      if(self.fingerprint_N=="max"):
        S = self.s_values
      else:
        if(self.sample_mode == "random"):
          self.sample_array = np.random.choice(np.arange(self.fingerprint_N))
          S = self.s_values[self.sample_array]
        if(self.sample_mode == "first"): 
          self.sample_array = np.arange(0,self.fingerprint_N)
          S = self.s_values[self.sample_array]
      f.write("  S = array({})\n".format(list(S)))

      #print(S)
      #print("{}".format(list(S)))
      #exit()

      f.write("  ret = 0\n")
      for term in params:
        n_ = term_dict[term]["n"]
        if(self.fit_mode == 'log'): expression = term_dict[term]['logmoment']
        if(self.fit_mode == 'normal'): expression = term_dict[term]['moment']
        for subterm in range(n_):
          expression = expression.replace(keys[iterate+subterm],values[iterate+subterm],1)
        iterate+=n_
        f.write("  ret += {}\n".format(expression.replace("_s_","S")))
      f.write("  return ret\n")
      f.write("fingerprint = frompyfunc(fp,{},1)\n".format(self.N_terms))

    ## Also write a log derivate function
    if(True):
      with open("./Functions/"+key+"_logderivative.py","w") as f:
        for req in logderivative_requirements: f.write(req+"\n")
        f.write("from numpy import array, frompyfunc\n")
        ### N.B. c term will do nothing
        f.write("def fp({}):\n".format(",".join(["p{}".format(i) for i in range(self.N_terms)])))
        iterate = 0
        if(self.fingerprint_N=="max"):
          S = self.s_values
        #else:
        #  if(self.sample_mode == "random"):
        #    self.sample_array = np.random.choice(np.arange(self.fingerprint_N))
        #    S = self.s_values[self.sample_array]
        #  if(self.sample_mode == "first"): 
        #    self.sample_array = np.arange(0,self.fingerprint_N)
        #    S = self.s_values[self.sample_array]
        f.write("  S = array({})\n".format(list(S)))
        f.write("  ret = 0\n")
        for term in params:
          n_ = term_dict[term]["n"]
          #if(self.fit_mode == 'log'): expression = term_dict[term]['logmoment']
          #if(self.fit_mode == 'normal'): expression = term_dict[term]['moment']
          expression = term_dict[term]['logderivative']
          for subterm in range(n_):
            expression = expression.replace(keys[iterate+subterm],values[iterate+subterm],1)
            ## Add an exception to reference the already used variables using a keyword
            subterm_var = "_VAR"+str(subterm+1)+"_"
            #print(subterm_var, values[iterate+subterm]) 
            #if(subterm_var in expression): (If it's not there it won't be replaced)
            expression = expression.replace(subterm_var,values[iterate+subterm])
            #print(expression)
            #input()
          iterate+=n_
          f.write("  ret += {}\n".format(expression.replace("_s_","S")))
        f.write("  return ret\n")
        f.write("logderivative = frompyfunc(fp,{},1)\n".format(self.N_terms))
    
    ## Also write a log derivate 2 function
    if(True):
      with open("./Functions/"+key+"_logderivative2.py","w") as f:
        for req in logderivative2_requirements: f.write(req+"\n")
        f.write("from numpy import array, frompyfunc\n")
        ### N.B. c term will do nothing
        f.write("def fp({}):\n".format(",".join(["p{}".format(i) for i in range(self.N_terms)])))
        iterate = 0
        if(self.fingerprint_N=="max"):
          S = self.s_values
        #else:
        #  if(self.sample_mode == "random"):
        #    self.sample_array = np.random.choice(np.arange(self.fingerprint_N))
        #    S = self.s_values[self.sample_array]
        #  if(self.sample_mode == "first"): 
        #    self.sample_array = np.arange(0,self.fingerprint_N)
        #    S = self.s_values[self.sample_array]
        f.write("  S = array({})\n".format(list(S)))
        f.write("  ret = 0\n")
        for term in params:
          n_ = term_dict[term]["n"]
          #if(self.fit_mode == 'log'): expression = term_dict[term]['logmoment']
          #if(self.fit_mode == 'normal'): expression = term_dict[term]['moment']
          expression = term_dict[term]['logderivative2']
          for subterm in range(n_):
            expression = expression.replace(keys[iterate+subterm],values[iterate+subterm],1)
            ## Add an exception to reference the already used variables using a keyword
            subterm_var = "_VAR"+str(subterm+1)+"_"
            expression = expression.replace(subterm_var,values[iterate+subterm])
          iterate+=n_
          f.write("  ret += {}\n".format(expression.replace("_s_","S")))
        f.write("  return ret\n")
        f.write("logderivative2 = frompyfunc(fp,{},1)\n".format(self.N_terms))

  def set_fingerprint(self,fp_hash):
    self.fingerprint = fp_hash

    print(fp_hash)

    ## Add a list to collect results dictionaries under this has
    if(self.fingerprint not in self.results): self.results[self.fingerprint] = []
    
    hash_list = fp_hash.split(":")
    print(hash_list)
    name = hash_list[0]
    hash_mode = hash_list[1]
    self.fingerprint_N = hash_list[2]
    if(self.fingerprint_N!="max"): self.fingerprint_N = int(self.fingerprint_N)
    self.sample_mode = hash_mode ## Important for random samples


    count = [ term_dict[term]["n"] for term in hash_list[3:]]
    self.N_terms = np.sum(count) 
 
    print(count)
    print(self.N_terms)

    ## If we have seen this fingerprint before, just load up the previous file
    if(self.fingerprint in self.fingerprint_function_dict and hash_mode == 'first'):
      print(self.function)
      print(self.fingerprint_function_dict)
      self.function = self.fingerprint_function_dict[self.fingerprint]
      print(self.function)
      #with open(".\\Functions\\{}.py".format(self.function),"r") as f: flines = f.readlines()
      if('fingerprint' in globals().keys()): del globals()['fingerprint']
      if('logderivative' in globals().keys()): del globals()['logderivative']
      if('logderivative2' in globals().keys()): del globals()['logderivative']
      
      ## Executes the python in the custom file (i.e. a function)
      with open("./Functions/{}.py".format(self.function),"r") as f: flines = f.readlines()
      exec("".join(flines),globals())
      with open("./Functions/{}_logderivative.py".format(self.function),"r") as f: flines = f.readlines()
      exec("".join(flines),globals())
      with open("./Functions/{}_logderivative2.py".format(self.function),"r") as f: flines = f.readlines()
      exec("".join(flines),globals())

      print("Function is reset to {}!".format(self.function))
      #print("LOAD log derivative???")
      return

    ## Otherwise write a vectorized loss function in terms of the input data etc. 
    key = "".join([np.random.choice(string_list) for i in range(10)])

    ## Write the function and its log derivative
    self.write_function(key)

    ## Overwrite any existing function with the name 
    if('fingerprint' in globals().keys()): del globals()['fingerprint']
    if('logderivative' in globals().keys()): del globals()['logderivative']
    if('logderivative2' in globals().keys()): del globals()['logderivative']
    #with open(".\\Functions\\{}.py".format(key),"r") as f: flines = f.readlines()
    with open("./Functions/{}.py".format(key),"r") as f: flines = f.readlines()
    exec("".join(flines),globals())
    with open("./Functions/{}_logderivative.py".format(key),"r") as f: flines = f.readlines()
    exec("".join(flines),globals())
    with open("./Functions/{}_logderivative2.py".format(key),"r") as f: flines = f.readlines()
    exec("".join(flines),globals())

    self.fingerprint_function_dict[self.fingerprint] = key
    self.function = key

  ## Searching for a good starting point
  def preseed(self, num_samples, logd = False):
    p0 = np.random.uniform(low=-10,high=10,size=[num_samples, self.N_terms])
    #p0 = np.random.choice([np.sqrt(2),0.5,1,2,3], size = [num_samples, self.N_terms])


    if(self.fit_mode=="log"):
      if(logd == True):
        f1 = self.real_logd_loss
        f2 = self.imag_logd_loss
      else:
        f1 = self.real_log_loss
        f2 = self.complex_log_loss
    if(self.fit_mode == "normal"):
      print("Normal BFGS not currently supported!")
      exit()
    losses = [self.real_log_loss(q) for q in p0]
    print(losses)
    print(np.amin(losses))
    print(np.amax(losses))
    amin = np.argmin(losses)
    print(amin)
    print(p0[amin])

    exit()



  ## A gradient based approach to get the optimial parameters for a given fingerprint
  def BFGS(self,p0=None, order=0):
    #self.BFGS_derivative_order = derivative_order
    #if(p0==None): p0=np.random.random(self.N_terms)
    if(p0==None): p0=np.random.uniform(low=-1,high=1,size=self.N_terms)
    if(self.fit_mode=="log"):
      f1 = self.real_log_loss
      f2 = self.complex_log_loss
    if(self.fit_mode == "normal"):
      print("Normal BFGS not currently supported!")
      exit()

    res = minimize(f1, x0=p0, args = (order), method = "BFGS", tol = 1e-6)
    res = minimize(f2, x0=res.x, args = (order), method = "BFGS", tol = 1e-8)
    loss = f2(res.x, order)
    self.register(res.x,loss)
    return res.x, loss

  ## Store the results
  def register(self,params,loss):
    ## Best Solution (eventually have top k solutions BPQ)
    if(loss < self.best_loss):
      print("New record solution! {}\n{}".format(loss,self.fingerprint))
      self.best_loss = loss
      self.best_params = params
      self.best_fingerprint = self.fingerprint
      self.best_function = self.function

    descriptors = self.descriptors_from_fingerprint(self.fingerprint)
    self.results["descriptors"] = descriptors
    self.results[self.fingerprint].append({"params" : params, "loss" : loss})

  ## Convert params to a unifed set of descriptors
  ## This may be useful to train models that predict loss via function composition
  ## Consider a Free-Wilson type analysis
  def descriptors_from_fingerprint(self,fingerprint):
    fp_list = fingerprint.split(":")[3:]
    descriptors = []
    elements = ["c","c^s","linear-gamma","scale-gamma","shift-gamma"]
    descriptors = [ fp_list.count(e) for e in elements ]
    return descriptors

  ## Vectorised difference function
  def real_log_loss(self,p, order):
    if(order == 0): 
      A = fingerprint(*p)[0]
      B = np.abs(np.real(A)-self.real_logm)
      B = np.maximum(0.0,B-self.real_log_diff)
    if(order == 1): 
      A = logderivative(*p)[0]
      B = np.abs(np.real(A)-np.real(self.ratio))
      #B = np.maximum(0.0,B-self.real_log_diff)
    if(order == 2): 
      A = logderivative2(*p)[0]# - logderivative(*p)[0]**2
      B = np.abs(np.real(A)-np.real(self.ratio2)+np.real(self.ratio**2))
      #B = np.maximum(0.0,B-self.real_log_diff)
    return np.mean(B)
  
  ## Vectorised difference function
  def complex_log_loss(self,p,order):
    if(order == 0): 
      A = fingerprint(*p)
      B = np.abs(np.real(A)-self.real_logm)
      B = np.maximum(0.0,B-self.real_log_diff)
      C = np.abs(wrap(np.imag(A)-self.imag_logm))
      C = np.maximum(0.0,C-self.imag_log_diff)
    if(order == 1): 
      A = logderivative(*p)
      B = np.abs(np.real(A)-np.real(self.ratio))
      C = np.abs(wrap(np.imag(A)-np.imag(self.ratio)))
    if(order == 2): 
      A = logderivative2(*p)# - logderivative(*p)**2
      B = np.abs(np.real(A)-np.real(self.ratio2)+np.real(self.ratio**2))
      C = np.abs(wrap(np.imag(A)-np.imag(self.ratio2)+np.imag(self.ratio**2)))
    return np.mean(B+C)

  def set_mode(self,mode): 
    if(mode not in ["log","normal"]):
      print("Error: valid mode choices are 'log' or 'normal' for logmoments or moments respectively!")
      exit()
    self.fit_mode = mode

  def summarise(self):
    results = self.results
    keys = self.results.keys()
    losses = [[j["loss"] for j in results[key]] for key in keys]
    minimum_loss = np.amin(losses,axis=1)
    for i,j in zip(keys,minimum_loss):
      print(i,j)

  def point_evaluation(self, q, order = 0):
    return self.real_log_loss(q, order), self.complex_log_loss(q, order)

  ## A function which suggests possible closed forms
  def speculate(self, samples = None, k = 4):
    print("Best result is: ")
    print("- Fingerprint: ",self.best_fingerprint)
    print("- Parameters: ",self.best_params)
    print("- Loss: ",self.best_loss)
    terms_list = self.function_terms_dict[self.best_function]

    print("Best Function ID: ",self.best_function)
    print("Log Space Function Expression: ~~~~")
    with open("Functions/{}.py".format(self.best_function),"r") as ff:
      flines = [i.strip() for i in ff.readlines()]
      flines = [i for i in flines if "ret " in i]
      for i in flines: 
        print(i)
    print("~~~~~~~~~~~~~~~~~~~~~~~~~")

    ## A place to store lists of samples
    sample_dict = {i : [] for i in terms_list.keys()}

    ## For each parameter get the correct list:KDTree pair
    for par,value in zip(terms_list.keys(),self.best_params):
      term_type = terms_list[par]
      ## Could do a search within  tolerance or just do nearest neighbours

      ## For the parameters that were fixed to squared to keep 
      if( term_type == "_sqrtnotzero_"):
        print("*** ",par,"**2 :",term_type," ***")
        distances, IDs = global_trees_dict[term_type].query([np.abs(value**2)],k)
      else:
        print("*** ",par,":",term_type," ***")
        distances, IDs = global_trees_dict[term_type].query([np.abs(value)],k)

      dict_IDs = [ list(global_constants_dict[term_type].keys())[i] for i in IDs]

      ## Use softmax to get probabilities?
      d_sum = np.sum([ np.exp(-d/np.mean(distances)) for d in distances ])
      probs = [ np.exp(-d/np.mean(distances))/d_sum for d in distances ]


      #### ADD SOMETHING LIKE if _sqrtnotzero_, just do 'best', or historical values, or random...
      #### Later we might want to make a comprehensive dict of values like 2^(-7/2), but could be huge...
      #### For gamma linear just do the best few
      if( term_type == "_sqrtnotzero_" ):
        sample_dict[par] = [value for i in range(samples)]        
      else:
        samp = np.random.choice([i for i in range(k)], size = samples, p = probs)
        sample_dict[par] = [ dict_IDs[j] for j in samp]
        sample_dict[par] = [global_constants_dict[term_type][q] for q in sample_dict[par]]

      for i, delta, p in zip(dict_IDs,distances, probs):
        delta = np.round(delta, decimals = 8)
        p = np.round(p, decimals = 3)
        ##i = i.replace("/1","")
        if( term_type == "_sqrtnotzero_"):
          print(value**2,"~","{}".format("-" if value < 0 else "")+i," (Delta = {}, p = {})".format(delta,p))
        else:
          print(value,"~","{}".format("-" if value < 0 else "")+i," (Delta = {}, p = {})".format(delta,p))
      
      print("*** ","~~~"," ***") 

    p0 = [np.sqrt(2**(-7/2)/3),np.sqrt(2**(1/2)),9/2,1/2]
    print("Ideal (test)--> ", p0)
    #if(samples == None): return []

    samp_points = []
    ## Generate samples
    #print(sample_dict)
    samples = np.vstack(list(sample_dict.values())).T
    samples = np.unique(samples,axis = 0)
    #print(samples.shape)
    return samples
    #exit()


## A function that helps
def gen_fpdict(parlist,N='max',mode='first',name=''):
  hash_string = ":".join(sorted(parlist))
  #print(kwargs)
  #if("name" not in kwargs.keys()): name = ''
  #else: name = kwargs["name"]
  #if("mode" not in kwargs.keys()): mode = 'first'
  #else: mode = kwargs["mode"]
  #if("N" not in kwargs.keys()): N = 'max'
  #else: N = kwargs["N"]
  if(mode not in['first','random']):  
    print("Error: gen_fpdict mode can only be 'first' or 'random' (or blank===first)!")
    exit()
  hash_string = ":".join([name,mode,str(N),hash_string])
  return hash_string



#### Working Example... ####
EE = ExactEstimator("Simple_Exponential", folder = "Simple_Exponential")
EE.set_fingerprint( gen_fpdict(['linear-gamma']))
for i in range(100): 
  EE.BFGS(order=2)
  print("{}%".format(100*(i+1)/n_bfgs),flush=True)
EE.speculate(samples = 1, k = 4)
############################

exit()

##
EE = ExactEstimator("Disk_Line_Picking", folder = "Disk_Line_Picking")
#EE = ExactEstimator("Chi_Distribution", folder = "Chi_Distribution")
fps = [gen_fpdict(['c','c^s','linear-gamma','linear-gamma','neg-linear-gamma','neg-linear-gamma'])]
#fps = [gen_fpdict(['c','c^s','linear-gamma'])]
for k in fps:
  print("Setting Fingerprint: ",k)
  EE.set_fingerprint(k)

  ## Do a bit of a random search
  ##EE.preseed(1000, logd = True)

  ##exit()
  n_bfgs = 50
  for i in range(n_bfgs):
    EE.BFGS(order = 1)
    print("{}%".format(100*(i+1)/n_bfgs),flush=True)

print("Completed!")
EE.speculate(samples = 1, k = 4)

optimal_points = [2/np.pi**(1/4),np.sqrt(2),1,1/2,1,1,2,1,5/2,1/2]
#optimal_points = [np.sqrt(2**(-7/2)/3),np.sqrt(2**(1/2)),9/2,1/2]
print(optimal_points)
print(np.array(optimal_points).shape)


print("Theoretical Best!")
loss = EE.point_evaluation(optimal_points, order = 0)
print("log phi(s)",loss,optimal_points)
loss = EE.point_evaluation(optimal_points, order = 1)
print("D_s log phi(s)",loss,optimal_points)
loss = EE.point_evaluation(optimal_points, order = 2)
print("D_s D_s log phi(s)",loss,optimal_points)

plots(EE.s_values,EE.logmoments,fingerprint(*optimal_points))
plots(EE.s_values,EE.ratio,logderivative(*optimal_points))
plots(EE.s_values,EE.ratio2-EE.ratio**2,logderivative2(*optimal_points))

print("\n~~~ Numerical Best! ~~~")
loss = EE.point_evaluation(EE.best_params, order = 0)
print("log phi(s)",loss,EE.best_params)
loss = EE.point_evaluation(EE.best_params, order = 1)
print("D_s log phi(s)",loss,EE.best_params)
loss = EE.point_evaluation(EE.best_params, order = 2)
print("D_s D_s log phi(s)",loss,EE.best_params)

pp = EE.best_params
plots(EE.s_values,EE.logmoments,fingerprint(*pp))
plots(EE.s_values,EE.ratio,logderivative(*pp))
plots(EE.s_values,EE.ratio2-EE.ratio**2,logderivative2(*pp))

exit()


#########################################
## Start the code here pointing to a file
EE = ExactEstimator("Chi_Distribution", folder = "Chi_Distribution")

fps=[
#gen_fpdict(['c','linear-gamma']),
#gen_fpdict(['c^s','linear-gamma']),
gen_fpdict(['c','c^s','linear-gamma']),
#gen_fpdict(['c','c^s','neg-linear-gamma']),
#gen_fpdict(['c','c^s','linear-gamma','neg-linear-gamma']),
#gen_fpdict(['c','c^s','linear-gamma','neg-linear-gamma','linear-gamma']),
#gen_fpdict(['c','c^s','linear-gamma','neg-linear-gamma','neg-linear-gamma']),
#gen_fpdict(['c','c^s','linear-gamma','linear-gamma','neg-linear-gamma','neg-linear-gamma']),
#gen_fpdict(['c','c^s','linear-gamma','linear-gamma','linear-gamma','neg-linear-gamma','neg-linear-gamma','neg-linear-gamma']),
#gen_fpdict(['c','linear-gamma','linear-gamma','neg-linear-gamma','neg-linear-gamma']),
#gen_fpdict(['c','linear-gamma','linear-gamma','linear-gamma','neg-linear-gamma','neg-linear-gamma','neg-linear-gamma']),
#gen_fpdict(['c','c^s','linear-gamma','neg-linear-gamma']),
#gen_fpdict(['c','c^s','linear-gamma','neg-linear-gamma','P1']),
#gen_fpdict(['c','c^s','linear-gamma','neg-linear-gamma','P2']),
#gen_fpdict(['c','c^s','2F1'],N=4),
#gen_fpdict(['c','c^s','2F1','linear-gamma'],N=4),
#gen_fpdict(['c','c^s','2F1','linear-gamma','neg-linear-gamma'],N=4)
]
#fp_x = gen_fpdict(['c','c^s','G-1-0-1-1'])   ### Look up how to do kwargs properly...

## Looping over fingerprints to see which is best
for k in fps:
  print("Setting Fingerprint: ",k)
  EE.set_fingerprint(k)
  n_bfgs = 300
  for i in range(n_bfgs):
    EE.BFGS()
    print("{}%".format(100*(i+1)/n_bfgs),flush=True)

print("Completed!")

## Get a few samples
samples_to_try = EE.speculate(samples = 100, k =4)

## Constrains that normalisation is true (s=1) --> Integral = 1
for q in samples_to_try:
  loss = EE.point_evaluation(q)
  print(q, loss)


EE.set_fingerprint(EE.best_fingerprint)

print(EE.best_params)
#plots(EE.s_values,EE.logmoments,fingerprint(*optimal_params))
plots(EE.s_values,EE.logmoments,fingerprint(*EE.best_params))

##print(EE.__dict__)
print(EE.results)

best_loss = EE.point_evaluation(EE.best_params)
print("Best Param Loss:", best_loss)
optimal_params = [np.sqrt(2**(-7/2)/3),np.sqrt(2**(1/2)),9/2,1/2]
best_loss = EE.point_evaluation(optimal_params)
print("Theoretical Best Loss:", best_loss)

### Consider a routine to enforce certain parameter values...




exit()






exit()
EE.set_fingerprint(fp_2)
for i in range(10): EE.BFGS()
EE.set_fingerprint(fp_3)
for i in range(10): EE.BFGS()

res = EE.results
print(res)

EE.summarise()

EE.set_mode('log')






